{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b96676",
   "metadata": {},
   "source": [
    "# A2. Graph API å–ã‚Šè¾¼ã¿ - å®Ÿè¡Œ\n",
    "\n",
    "## æ¦‚è¦\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€SharePoint ã‹ã‚‰å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã€Azure AI Search ã«å–ã‚Šè¾¼ã¿ã¾ã™ã€‚\n",
    "\n",
    "### å‡¦ç†ãƒ•ãƒ­ãƒ¼\n",
    "\n",
    "1. **ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ**: ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢å¯¾å¿œã®ã‚¹ã‚­ãƒ¼ãƒå®šç¾©\n",
    "2. **ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—**: Graph API ã§ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—\n",
    "3. **æœ¬æ–‡æŠ½å‡º**: PDF, Word, Excel, PowerPoint ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º\n",
    "4. **ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²**: é•·æ–‡ã‚’é©åˆ‡ãªã‚µã‚¤ã‚ºã«åˆ†å‰²\n",
    "5. **åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ**: Azure OpenAI ã§åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)\n",
    "6. **ACL å‡¦ç†**: ãƒ¦ãƒ¼ã‚¶ãƒ¼/ã‚°ãƒ«ãƒ¼ãƒ—æ¨©é™ã®å–å¾—\n",
    "7. **ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: Azure AI Search ã«ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "### æ‰€è¦æ™‚é–“\n",
    "\n",
    "- ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚„å®¹é‡ã«ã‚ˆã‚Šå¤‰å‹•ã—ã¾ã™ãŒã€100ãƒ•ã‚¡ã‚¤ãƒ«ç¨‹åº¦ã§ 10ã€œ30åˆ† ç¨‹åº¦\n",
    "- åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨è¿½åŠ ã§æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81059e",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒã®åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32243ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import msal\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Azure Search\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch\n",
    ")\n",
    "\n",
    "# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
    "from utils import (\n",
    "    TextExtractor,\n",
    "    TextChunker,\n",
    "    create_document_id,\n",
    "    extract_acl_from_permissions\n",
    ")\n",
    "\n",
    "# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿\n",
    "load_dotenv(Path(\"..\") / \".env\")\n",
    "\n",
    "# å‰å›ã®è¨­å®šã‚’èª­ã¿è¾¼ã¿\n",
    "config_path = Path(\"..\") / \"config.json\"\n",
    "if config_path.exists():\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = json.load(f)\n",
    "    print(\"âœ… å‰å›ã®è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
    "else:\n",
    "    print(\"âŒ config.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚A1_graph_ingest_setup.ipynb ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "    raise FileNotFoundError(\"config.json not found\")\n",
    "\n",
    "print(f\"\\nã‚µã‚¤ãƒˆ: {config['site_info']['name']}\")\n",
    "print(f\"å¯¾è±¡ãƒ‰ãƒ©ã‚¤ãƒ–æ•°: {len(config['target_drives'])}\")\n",
    "print(f\"ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {config['chunk_size']} æ–‡å­—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69692c2b",
   "metadata": {},
   "source": [
    "## 2. Graph API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34046c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ (A1 ã‹ã‚‰å†åˆ©ç”¨)\n",
    "class GraphAPIClient:\n",
    "    def __init__(self, tenant_id: str, client_id: str, client_secret: str):\n",
    "        self.tenant_id = tenant_id\n",
    "        self.client_id = client_id\n",
    "        self.authority = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "        self.base_url = \"https://graph.microsoft.com/v1.0\"\n",
    "        \n",
    "        self.app = msal.ConfidentialClientApplication(\n",
    "            client_id,\n",
    "            authority=self.authority,\n",
    "            client_credential=client_secret\n",
    "        )\n",
    "        self.token = None\n",
    "    \n",
    "    def get_token(self) -> str:\n",
    "        result = self.app.acquire_token_for_client(\n",
    "            scopes=[\"https://graph.microsoft.com/.default\"]\n",
    "        )\n",
    "        if \"access_token\" in result:\n",
    "            self.token = result[\"access_token\"]\n",
    "            return self.token\n",
    "        else:\n",
    "            raise Exception(f\"Token error: {result.get('error_description')}\")\n",
    "    \n",
    "    def get_headers(self) -> Dict[str, str]:\n",
    "        if not self.token:\n",
    "            self.get_token()\n",
    "        return {\n",
    "            \"Authorization\": f\"Bearer {self.token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def get(self, endpoint: str, params: Optional[Dict] = None) -> Dict:\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        response = requests.get(url, headers=self.get_headers(), params=params)\n",
    "        \n",
    "        # ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ (429 å¯¾ç­–)\n",
    "        retry_count = 0\n",
    "        while response.status_code == 429 and retry_count < 3:\n",
    "            retry_after = int(response.headers.get(\"Retry-After\", 5))\n",
    "            print(f\"âš ï¸  Rate limited. Waiting {retry_after} seconds...\")\n",
    "            time.sleep(retry_after)\n",
    "            response = requests.get(url, headers=self.get_headers(), params=params)\n",
    "            retry_count += 1\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ\n",
    "graph_client = GraphAPIClient(\n",
    "    tenant_id=os.getenv(\"GRAPH_TENANT_ID\"),\n",
    "    client_id=os.getenv(\"GRAPH_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"GRAPH_CLIENT_SECRET\")\n",
    ")\n",
    "\n",
    "print(\"âœ… Graph API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61d8d78",
   "metadata": {},
   "source": [
    "## 3. Azure AI Search ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=search_endpoint,\n",
    "    credential=AzureKeyCredential(search_key)\n",
    ")\n",
    "\n",
    "# ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã‚’ä½¿ã†ã‹ã©ã†ã‹\n",
    "use_vector_search = bool(os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "vector_dimensions = 3072  # text-embedding-ada-002 ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ\n",
    "\n",
    "if use_vector_search:\n",
    "    print(f\"âœ… ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã™ (æ¬¡å…ƒæ•°: {vector_dimensions})\")\n",
    "else:\n",
    "    print(\"âš ï¸  Azure OpenAI ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã¯ç„¡åŠ¹ã§ã™\")\n",
    "\n",
    "print(f\"\\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å: {index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e16b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String, sortable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SimpleField(name=\"url\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SimpleField(name=\"path\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SimpleField(name=\"site\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SimpleField(name=\"library\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SimpleField(name=\"contentType\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "    SimpleField(name=\"fileExtension\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "    SimpleField(name=\"lastModified\", type=SearchFieldDataType.DateTimeOffset, sortable=True, filterable=True),\n",
    "    SimpleField(name=\"createdBy\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SimpleField(name=\"modifiedBy\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SimpleField(name=\"size\", type=SearchFieldDataType.Int64, sortable=True, filterable=True),\n",
    "    SimpleField(name=\"chunkIndex\", type=SearchFieldDataType.Int32, filterable=True),\n",
    "    SimpleField(name=\"aclUsers\", type=SearchFieldDataType.Collection(SearchFieldDataType.String), filterable=True),\n",
    "    SimpleField(name=\"aclGroups\", type=SearchFieldDataType.Collection(SearchFieldDataType.String), filterable=True),\n",
    "]\n",
    "\n",
    "# ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢è¨­å®š\n",
    "vector_search_config = None\n",
    "if use_vector_search:\n",
    "    # ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ \n",
    "    fields.append(\n",
    "        SearchField(\n",
    "            name=\"contentVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=vector_dimensions,\n",
    "            vector_search_profile_name=\"default-vector-profile\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢è¨­å®š\n",
    "    vector_search_config = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"default-hnsw\",\n",
    "                parameters={\n",
    "                    \"m\": 4,\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500,\n",
    "                    \"metric\": \"cosine\"\n",
    "                }\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"default-vector-profile\",\n",
    "                algorithm_configuration_name=\"default-hnsw\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢è¨­å®š\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"default\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ\n",
    "index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search_config,\n",
    "    semantic_search=semantic_search\n",
    ")\n",
    "\n",
    "try:\n",
    "    # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)\n",
    "    try:\n",
    "        index_client.delete_index(index_name)\n",
    "        print(f\"âš ï¸  æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ\")\n",
    "        time.sleep(2)  # å‰Šé™¤å®Œäº†ã‚’å¾…ã¤\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # æ–°è¦ä½œæˆ\n",
    "    result = index_client.create_index(index)\n",
    "    print(f\"\\nâœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸ: {result.name}\")\n",
    "    print(f\"   ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°: {len(result.fields)}\")\n",
    "    if use_vector_search:\n",
    "        print(f\"   ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢: æœ‰åŠ¹ ({vector_dimensions} æ¬¡å…ƒ)\")\n",
    "    print(f\"   ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢: æœ‰åŠ¹\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e73838",
   "metadata": {},
   "source": [
    "## 4. Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æº–å‚™ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b96fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = None\n",
    "embedding_deployment = None\n",
    "\n",
    "if use_vector_search:\n",
    "    try:\n",
    "        from openai import AzureOpenAI\n",
    "        \n",
    "        openai_client = AzureOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\")\n",
    "        )\n",
    "        embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "        \n",
    "        # ãƒ†ã‚¹ãƒˆ\n",
    "        test_response = openai_client.embeddings.create(\n",
    "            input=\"test\",\n",
    "            model=embedding_deployment\n",
    "        )\n",
    "        vector_dimensions = len(test_response.data[0].embedding)\n",
    "        \n",
    "        print(f\"âœ… Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ\")\n",
    "        print(f\"   ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ: {embedding_deployment}\")\n",
    "        print(f\"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {vector_dimensions}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Azure OpenAI ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}\")\n",
    "        print(\"   ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ãªã—ã§ç¶šè¡Œã—ã¾ã™\")\n",
    "        use_vector_search = False\n",
    "else:\n",
    "    print(\"âš ï¸  ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã¯ç„¡åŠ¹ã§ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f86804",
   "metadata": {},
   "source": [
    "## 5. ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã¨å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a391957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files_recursive(\n",
    "    graph_client: GraphAPIClient,\n",
    "    drive_id: str,\n",
    "    folder_id: str = \"root\",\n",
    "    supported_extensions: List[str] = None,\n",
    "    max_files: int = 100\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ãƒ‰ãƒ©ã‚¤ãƒ–å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«å–å¾—\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    \n",
    "    def traverse(item_id: str, current_path: str = \"\"):\n",
    "        if len(all_files) >= max_files:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            endpoint = f\"/drives/{drive_id}/items/{item_id}/children\"\n",
    "            response = graph_client.get(endpoint, params={\"$top\": 200})\n",
    "            \n",
    "            for item in response.get(\"value\", []):\n",
    "                if len(all_files) >= max_files:\n",
    "                    break\n",
    "                \n",
    "                item_name = item[\"name\"]\n",
    "                item_path = f\"{current_path}/{item_name}\" if current_path else item_name\n",
    "                \n",
    "                if \"folder\" in item:\n",
    "                    # ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆã¯å†å¸°\n",
    "                    traverse(item[\"id\"], item_path)\n",
    "                elif \"file\" in item:\n",
    "                    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ\n",
    "                    ext = Path(item_name).suffix.lower()\n",
    "                    \n",
    "                    if supported_extensions is None or ext in supported_extensions:\n",
    "                        file_info = {\n",
    "                            \"id\": item[\"id\"],\n",
    "                            \"name\": item_name,\n",
    "                            \"path\": item_path,\n",
    "                            \"extension\": ext,\n",
    "                            \"size\": item.get(\"size\", 0),\n",
    "                            \"webUrl\": item.get(\"webUrl\", \"\"),\n",
    "                            \"lastModified\": item.get(\"lastModifiedDateTime\", \"\"),\n",
    "                            \"createdBy\": item.get(\"createdBy\", {}).get(\"user\", {}).get(\"displayName\", \"\"),\n",
    "                            \"modifiedBy\": item.get(\"lastModifiedBy\", {}).get(\"user\", {}).get(\"displayName\", \"\"),\n",
    "                        }\n",
    "                        all_files.append(file_info)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  ãƒ•ã‚©ãƒ«ãƒ€ {current_path} ã®æ¢ç´¢ã§ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}\")\n",
    "    \n",
    "    traverse(folder_id)\n",
    "    return all_files\n",
    "\n",
    "# å…¨ãƒ‰ãƒ©ã‚¤ãƒ–ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—\n",
    "all_files = []\n",
    "supported_exts = config[\"supported_extensions\"]\n",
    "max_files = config[\"max_files\"]\n",
    "\n",
    "print(\"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã¦ã„ã¾ã™...\\n\")\n",
    "\n",
    "for drive in config[\"target_drives\"]:\n",
    "    print(f\"ğŸ“ {drive['name']}\")\n",
    "    files = get_all_files_recursive(\n",
    "        graph_client,\n",
    "        drive[\"id\"],\n",
    "        \"root\",\n",
    "        supported_exts,\n",
    "        max_files - len(all_files)\n",
    "    )\n",
    "    \n",
    "    for f in files:\n",
    "        f[\"drive_id\"] = drive[\"id\"]\n",
    "        f[\"drive_name\"] = drive[\"name\"]\n",
    "    \n",
    "    all_files.extend(files)\n",
    "    print(f\"   å–å¾—: {len(files)} ä»¶\\n\")\n",
    "    \n",
    "    if len(all_files) >= max_files:\n",
    "        break\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"âœ… åˆè¨ˆ {len(all_files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã¾ã—ãŸ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# çµ±è¨ˆ\n",
    "total_size = sum(f[\"size\"] for f in all_files) / (1024 * 1024)\n",
    "print(f\"\\nåˆè¨ˆã‚µã‚¤ã‚º: {total_size:.2f} MB\")\n",
    "\n",
    "# æ‹¡å¼µå­åˆ¥é›†è¨ˆ\n",
    "ext_count = {}\n",
    "for f in all_files:\n",
    "    ext = f[\"extension\"]\n",
    "    ext_count[ext] = ext_count.get(ext, 0) + 1\n",
    "\n",
    "print(\"\\næ‹¡å¼µå­åˆ¥:\")\n",
    "for ext, count in sorted(ext_count.items()):\n",
    "    print(f\"  {ext}: {count} ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de414a34",
   "metadata": {},
   "source": [
    "## 6. ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc997fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(\n",
    "    graph_client: GraphAPIClient,\n",
    "    file_info: Dict,\n",
    "    site_info: Dict,\n",
    "    chunker: TextChunker,\n",
    "    openai_client = None,\n",
    "    embedding_deployment: str = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’è¿”ã™\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "        download_url = f\"/drives/{file_info['drive_id']}/items/{file_info['id']}/content\"\n",
    "        content_response = requests.get(\n",
    "            f\"{graph_client.base_url}{download_url}\",\n",
    "            headers=graph_client.get_headers(),\n",
    "            allow_redirects=True\n",
    "        )\n",
    "        content_response.raise_for_status()\n",
    "        content_bytes = content_response.content\n",
    "        \n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º\n",
    "        text = TextExtractor.extract(content_bytes, file_info[\"extension\"])\n",
    "        \n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return []  # ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "        \n",
    "        # ACLå–å¾— (æœ¬ç•ªç’°å¢ƒå®Ÿè£…)\n",
    "        acl_users = []\n",
    "        acl_groups = []\n",
    "        \n",
    "        try:\n",
    "            # Permissions APIã‚’å‘¼ã³å‡ºã—ã¦ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’å–å¾—\n",
    "            permissions_endpoint = f\"/drives/{file_info['drive_id']}/items/{file_info['id']}/permissions\"\n",
    "            permissions_response = graph_client.get(permissions_endpoint)\n",
    "            permissions = permissions_response.get(\"value\", [])\n",
    "            \n",
    "            # ACLã‚’æŠ½å‡º\n",
    "            acl_users, acl_groups = extract_acl_from_permissions(permissions)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Permissionå–å¾—ã«å¤±æ•—ã—ã¦ã‚‚å‡¦ç†ã‚’ç¶šè¡Œ\n",
    "            print(f\"âš ï¸  Permissionå–å¾—ã‚¨ãƒ©ãƒ¼ ({file_info['name']}): {str(e)[:80]}\")\n",
    "        \n",
    "        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²\n",
    "        chunks = chunker.split_text(text, metadata=file_info)\n",
    "        \n",
    "        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ\n",
    "        documents = []\n",
    "        for chunk in chunks:\n",
    "            doc_id = create_document_id(\n",
    "                site_info[\"id\"],\n",
    "                file_info[\"drive_id\"],\n",
    "                file_info[\"id\"],\n",
    "                chunk.chunk_index\n",
    "            )\n",
    "            \n",
    "            doc = {\n",
    "                \"id\": doc_id,\n",
    "                \"title\": file_info[\"name\"],\n",
    "                \"content\": chunk.text,\n",
    "                \"url\": file_info[\"webUrl\"],\n",
    "                \"path\": file_info[\"path\"],\n",
    "                \"site\": site_info[\"name\"],\n",
    "                \"library\": file_info[\"drive_name\"],\n",
    "                \"contentType\": file_info[\"extension\"][1:] if file_info[\"extension\"] else \"unknown\",\n",
    "                \"fileExtension\": file_info[\"extension\"],\n",
    "                \"lastModified\": file_info[\"lastModified\"],\n",
    "                \"createdBy\": file_info[\"createdBy\"],\n",
    "                \"modifiedBy\": file_info[\"modifiedBy\"],\n",
    "                \"size\": file_info[\"size\"],\n",
    "                \"chunkIndex\": chunk.chunk_index,\n",
    "                \"aclUsers\": acl_users,\n",
    "                \"aclGroups\": acl_groups,\n",
    "            }\n",
    "            \n",
    "            # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)\n",
    "            if openai_client and embedding_deployment:\n",
    "                try:\n",
    "                    response = openai_client.embeddings.create(\n",
    "                        input=chunk.text[:8000],  # ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–\n",
    "                        model=embedding_deployment\n",
    "                    )\n",
    "                    doc[\"contentVector\"] = response.data[0].embedding\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼ ({file_info['name']}): {str(e)[:50]}\")\n",
    "            \n",
    "            documents.append(doc)\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({file_info['name']}): {str(e)[:100]}\")\n",
    "        return []\n",
    "\n",
    "# ãƒãƒ£ãƒ³ã‚«ãƒ¼ã®æº–å‚™\n",
    "chunker = TextChunker(\n",
    "    chunk_size=config[\"chunk_size\"],\n",
    "    chunk_overlap=config[\"chunk_overlap\"]\n",
    ")\n",
    "\n",
    "print(\"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "print(f\"   ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {config['chunk_size']} æ–‡å­—\")\n",
    "print(f\"   ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: {config['chunk_overlap']} æ–‡å­—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†\n",
    "all_documents = []\n",
    "failed_files = []\n",
    "\n",
    "print(\"\\nãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ã„ã¾ã™...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for file_info in tqdm(all_files, desc=\"å‡¦ç†ä¸­\"):\n",
    "    docs = process_file(\n",
    "        graph_client,\n",
    "        file_info,\n",
    "        config[\"site_info\"],\n",
    "        chunker,\n",
    "        openai_client,\n",
    "        embedding_deployment\n",
    "    )\n",
    "    \n",
    "    if docs:\n",
    "        all_documents.extend(docs)\n",
    "    else:\n",
    "        failed_files.append(file_info[\"name\"])\n",
    "    \n",
    "    # APIåˆ¶é™å¯¾ç­–: å°‘ã—å¾…ã¤\n",
    "    if use_vector_search:\n",
    "        time.sleep(0.1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"âœ… å‡¦ç†å®Œäº†\")\n",
    "print(f\"   æˆåŠŸ: {len(all_files) - len(failed_files)} / {len(all_files)} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "print(f\"   ç”Ÿæˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(all_documents)} ä»¶\")\n",
    "if failed_files:\n",
    "    print(f\"   å¤±æ•—: {len(failed_files)} ä»¶\")\n",
    "    print(f\"   å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«: {failed_files[:5]}\" + (\" ...\" if len(failed_files) > 5 else \"\"))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da9509",
   "metadata": {},
   "source": [
    "## 7. Azure AI Search ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b92822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(search_key)\n",
    ")\n",
    "\n",
    "# æ—¢å­˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒªã‚¢ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)\n",
    "print(\"æ—¢å­˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒªã‚¢ã—ã¦ã„ã¾ã™...\")\n",
    "\n",
    "try:\n",
    "    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†…ã®å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢\n",
    "    search_results = search_client.search(\n",
    "        search_text=\"*\",\n",
    "        top=1000,  # ä¸€åº¦ã«å–å¾—ã™ã‚‹æœ€å¤§æ•°\n",
    "        select=[\"id\"]\n",
    "    )\n",
    "    \n",
    "    existing_docs = [{\"id\": doc[\"id\"]} for doc in search_results]\n",
    "    \n",
    "    if existing_docs:\n",
    "        print(f\"æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {len(existing_docs)} ä»¶ã‚’å‰Šé™¤ä¸­...\")\n",
    "        \n",
    "        # ãƒãƒƒãƒå‰Šé™¤\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(existing_docs), batch_size):\n",
    "            batch = existing_docs[i:i + batch_size]\n",
    "            try:\n",
    "                search_client.delete_documents(documents=batch)\n",
    "                print(f\"  å‰Šé™¤æ¸ˆã¿: {min(i + batch_size, len(existing_docs))} / {len(existing_docs)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}\")\n",
    "        \n",
    "        # å‰Šé™¤å®Œäº†ã‚’å¾…ã¤\n",
    "        time.sleep(3)\n",
    "        print(\"âœ… æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‰Šé™¤ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "    else:\n",
    "        print(\"å‰Šé™¤å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤ã§ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}\")\n",
    "    print(\"   æ–°ã—ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ ã—ã¾ã™\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985821a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "batch_size = 100  # 1å›ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§é€ä¿¡ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°\n",
    "total_uploaded = 0\n",
    "total_failed = 0\n",
    "\n",
    "print(\"\\nAzure AI Search ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in tqdm(range(0, len(all_documents), batch_size), desc=\"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­\"):\n",
    "    batch = all_documents[i:i + batch_size]\n",
    "    \n",
    "    try:\n",
    "        result = search_client.upload_documents(documents=batch)\n",
    "        \n",
    "        # çµæœã®ç¢ºèª\n",
    "        for res in result:\n",
    "            if res.succeeded:\n",
    "                total_uploaded += 1\n",
    "            else:\n",
    "                total_failed += 1\n",
    "                print(f\"âš ï¸  ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {res.key} - {res.error_message}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        total_failed += len(batch)\n",
    "        print(f\"âŒ ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}\")\n",
    "    \n",
    "    # ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°å¯¾ç­–\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†\")\n",
    "print(f\"   æˆåŠŸ: {total_uploaded} / {len(all_documents)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\")\n",
    "if total_failed > 0:\n",
    "    print(f\"   å¤±æ•—: {total_failed} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398eaa3",
   "metadata": {},
   "source": [
    "## 8. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b6441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆ\n",
    "time.sleep(3)  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ›´æ–°ã‚’å¾…ã¤\n",
    "\n",
    "try:\n",
    "    # ç°¡å˜ãªã‚«ã‚¦ãƒ³ãƒˆã‚¯ã‚¨ãƒª\n",
    "    results = search_client.search(\n",
    "        search_text=\"*\",\n",
    "        top=1,\n",
    "        include_total_count=True\n",
    "    )\n",
    "    \n",
    "    total_count = results.get_count()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“Š ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆ\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å: {index_name}\")\n",
    "    print(f\"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {total_count:,} ä»¶\")\n",
    "    print(f\"\\nå‡¦ç†çµæœ:\")\n",
    "    print(f\"  å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«: {len(all_files)} ä»¶\")\n",
    "    print(f\"  ç”Ÿæˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(all_documents)} ä»¶\")\n",
    "    print(f\"  å¹³å‡ãƒãƒ£ãƒ³ã‚¯æ•°/ãƒ•ã‚¡ã‚¤ãƒ«: {len(all_documents) / max(len(all_files), 1):.1f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2d465",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ä»¥ä¸‹ã‚’å®Œäº†ã—ã¾ã—ãŸ:\n",
    "\n",
    "âœ… Azure AI Search ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ  \n",
    "âœ… SharePoint ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—  \n",
    "âœ… ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã¨ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²  \n",
    "âœ… åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ç”Ÿæˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)  \n",
    "âœ… Azure AI Search ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰  \n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "**A3_graph_delta_sync.ipynb** ã«é€²ã‚“ã§ã€å·®åˆ†åŒæœŸã®ä»•çµ„ã¿ã‚’å­¦ã³ã¾ã™ã€‚\n",
    "\n",
    "### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "\n",
    "**ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼**:\n",
    "- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDãŒé‡è¤‡ã—ã¦ã„ãªã„ã‹ç¢ºèª\n",
    "- ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚ºãŒ 16MB ã‚’è¶…ãˆã¦ã„ãªã„ã‹ç¢ºèª\n",
    "\n",
    "**åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼**:\n",
    "- Azure OpenAI ã®ã‚¯ã‚©ãƒ¼ã‚¿ã‚’ç¢ºèª\n",
    "- å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒ 8192 ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¶…ãˆã¦ã„ãªã„ã‹ç¢ºèª\n",
    "\n",
    "**API ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°**:\n",
    "- `time.sleep()` ã®å€¤ã‚’èª¿æ•´\n",
    "- ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹\n",
    "\n",
    "### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "\n",
    "- **ä¸¦åˆ—å‡¦ç†**: `concurrent.futures` ã§è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒæ™‚å‡¦ç†\n",
    "- **ãƒãƒƒãƒã‚µã‚¤ã‚º**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´\n",
    "- **ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º**: æ¤œç´¢ç²¾åº¦ã¨å‡¦ç†é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
