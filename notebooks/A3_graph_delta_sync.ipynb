{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ead500",
   "metadata": {},
   "source": [
    "# A3. Graph API å·®åˆ†åŒæœŸ\n",
    "\n",
    "## æ¦‚è¦\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Microsoft Graph ã® **Delta Query API** ã‚’ä½¿ã£ã¦ã€SharePoint ã®å¤‰æ›´ã‚’åŠ¹ç‡çš„ã«æ¤œå‡ºã—ã€Azure AI Search ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã—ã¾ã™ã€‚\n",
    "\n",
    "### Delta Query ã¨ã¯\n",
    "\n",
    "Delta Query ã¯ã€å‰å›ã®åŒæœŸä»¥é™ã«å¤‰æ›´ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ  (è¿½åŠ /æ›´æ–°/å‰Šé™¤) ã®ã¿ã‚’å–å¾—ã§ãã‚‹ API ã§ã™ã€‚\n",
    "\n",
    "**ãƒ¡ãƒªãƒƒãƒˆ**:\n",
    "- âš¡ é«˜é€Ÿ: å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å‡¦ç†\n",
    "- ğŸ’° ã‚³ã‚¹ãƒˆå‰Šæ¸›: API å‘¼ã³å‡ºã—å›æ•°ã¨å‡¦ç†æ™‚é–“ã‚’å‰Šæ¸›\n",
    "- ğŸ”„ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§: å®šæœŸå®Ÿè¡Œã§å¸¸ã«æœ€æ–°çŠ¶æ…‹ã‚’ç¶­æŒ\n",
    "\n",
    "### å‡¦ç†ãƒ•ãƒ­ãƒ¼\n",
    "\n",
    "1. **Delta Token ã®å–å¾—**: å‰å›åŒæœŸæ™‚ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’èª­ã¿è¾¼ã¿\n",
    "2. **å¤‰æ›´ã®å–å¾—**: Delta API ã§å¤‰æ›´ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—\n",
    "3. **å¤‰æ›´ã®åˆ†é¡**: è¿½åŠ /æ›´æ–°/å‰Šé™¤ã‚’åˆ¤å®š\n",
    "4. **ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°**: \n",
    "   - è¿½åŠ /æ›´æ–°: A2 ã¨åŒã˜å‡¦ç†ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆâ†’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "   - å‰Šé™¤: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰è©²å½“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤\n",
    "5. **Token ã®ä¿å­˜**: æ¬¡å›åŒæœŸç”¨ã« Delta Token ã‚’ä¿å­˜\n",
    "\n",
    "### æ‰€è¦æ™‚é–“\n",
    "\n",
    "- åˆå›å®Ÿè¡Œ: 5ã€œ10åˆ† (Delta Token ã®åˆæœŸåŒ–)\n",
    "- 2å›ç›®ä»¥é™: æ•°ç§’ã€œæ•°åˆ† (å¤‰æ›´é‡ã«ä¾å­˜)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb9644",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒã®åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db7c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import msal\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Azure Search\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
    "from utils import (\n",
    "    TextExtractor,\n",
    "    TextChunker,\n",
    "    create_document_id,\n",
    "    extract_acl_from_permissions\n",
    ")\n",
    "\n",
    "# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿\n",
    "load_dotenv(Path(\"..\") / \".env\")\n",
    "\n",
    "# è¨­å®šã®èª­ã¿è¾¼ã¿\n",
    "config_path = Path(\"..\") / \"config.json\"\n",
    "if config_path.exists():\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = json.load(f)\n",
    "    print(\"âœ… è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
    "else:\n",
    "    print(\"âŒ config.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "    raise FileNotFoundError(\"config.json not found\")\n",
    "\n",
    "# Delta Token ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "delta_token_file = Path(\"..\") / os.getenv(\"DELTA_TOKEN_FILE\", \".delta_token.json\")\n",
    "\n",
    "print(f\"\\nã‚µã‚¤ãƒˆ: {config['site_info']['name']}\")\n",
    "print(f\"Delta Token ãƒ•ã‚¡ã‚¤ãƒ«: {delta_token_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db621530",
   "metadata": {},
   "source": [
    "## 2. Graph API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d470ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAPIClient:\n",
    "    def __init__(self, tenant_id: str, client_id: str, client_secret: str):\n",
    "        self.tenant_id = tenant_id\n",
    "        self.client_id = client_id\n",
    "        self.authority = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "        self.base_url = \"https://graph.microsoft.com/v1.0\"\n",
    "        \n",
    "        self.app = msal.ConfidentialClientApplication(\n",
    "            client_id,\n",
    "            authority=self.authority,\n",
    "            client_credential=client_secret\n",
    "        )\n",
    "        self.token = None\n",
    "    \n",
    "    def get_token(self) -> str:\n",
    "        result = self.app.acquire_token_for_client(\n",
    "            scopes=[\"https://graph.microsoft.com/.default\"]\n",
    "        )\n",
    "        if \"access_token\" in result:\n",
    "            self.token = result[\"access_token\"]\n",
    "            return self.token\n",
    "        else:\n",
    "            raise Exception(f\"Token error: {result.get('error_description')}\")\n",
    "    \n",
    "    def get_headers(self) -> Dict[str, str]:\n",
    "        if not self.token:\n",
    "            self.get_token()\n",
    "        return {\n",
    "            \"Authorization\": f\"Bearer {self.token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def get(self, endpoint: str, params: Optional[Dict] = None) -> Dict:\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        response = requests.get(url, headers=self.get_headers(), params=params)\n",
    "        \n",
    "        # ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯\n",
    "        retry_count = 0\n",
    "        while response.status_code == 429 and retry_count < 3:\n",
    "            retry_after = int(response.headers.get(\"Retry-After\", 5))\n",
    "            print(f\"âš ï¸  Rate limited. Waiting {retry_after} seconds...\")\n",
    "            time.sleep(retry_after)\n",
    "            response = requests.get(url, headers=self.get_headers(), params=params)\n",
    "            retry_count += 1\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def get_with_url(self, full_url: str) -> Dict:\n",
    "        \"\"\"ãƒ•ãƒ«URLã§ç›´æ¥ãƒªã‚¯ã‚¨ã‚¹ãƒˆ (Delta API ã® nextLink/deltaLink ç”¨)\"\"\"\n",
    "        response = requests.get(full_url, headers=self.get_headers())\n",
    "        \n",
    "        retry_count = 0\n",
    "        while response.status_code == 429 and retry_count < 3:\n",
    "            retry_after = int(response.headers.get(\"Retry-After\", 5))\n",
    "            print(f\"âš ï¸  Rate limited. Waiting {retry_after} seconds...\")\n",
    "            time.sleep(retry_after)\n",
    "            response = requests.get(full_url, headers=self.get_headers())\n",
    "            retry_count += 1\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ\n",
    "graph_client = GraphAPIClient(\n",
    "    tenant_id=os.getenv(\"GRAPH_TENANT_ID\"),\n",
    "    client_id=os.getenv(\"GRAPH_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"GRAPH_CLIENT_SECRET\")\n",
    ")\n",
    "\n",
    "print(\"âœ… Graph API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf0ecbd",
   "metadata": {},
   "source": [
    "## 3. Delta Token ã®ç®¡ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_delta_tokens() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Delta Token ã‚’èª­ã¿è¾¼ã‚€\n",
    "    ãƒ‰ãƒ©ã‚¤ãƒ–ã”ã¨ã« Token ã‚’ç®¡ç†\n",
    "    \"\"\"\n",
    "    if delta_token_file.exists():\n",
    "        with open(delta_token_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            tokens = json.load(f)\n",
    "        print(f\"âœ… Delta Token ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(tokens)} ãƒ‰ãƒ©ã‚¤ãƒ–)\")\n",
    "        return tokens\n",
    "    else:\n",
    "        print(\"â„¹ï¸  Delta Token ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ (åˆå›å®Ÿè¡Œ)\")\n",
    "        return {}\n",
    "\n",
    "def save_delta_tokens(tokens: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Delta Token ã‚’ä¿å­˜\n",
    "    \"\"\"\n",
    "    with open(delta_token_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(tokens, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… Delta Token ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {delta_token_file}\")\n",
    "\n",
    "# æ—¢å­˜ã® Token ã‚’èª­ã¿è¾¼ã¿\n",
    "delta_tokens = load_delta_tokens()\n",
    "\n",
    "print(\"\\nç¾åœ¨ã® Token çŠ¶æ…‹:\")\n",
    "for drive in config[\"target_drives\"]:\n",
    "    drive_id = drive[\"id\"]\n",
    "    has_token = drive_id in delta_tokens\n",
    "    status = \"âœ“\" if has_token else \"âœ—\"\n",
    "    print(f\"  {status} {drive['name']}: {'Token ã‚ã‚Š' if has_token else 'Token ãªã— (åˆå›åŒæœŸãŒå¿…è¦)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe046e",
   "metadata": {},
   "source": [
    "## 4. Delta API ã«ã‚ˆã‚‹å¤‰æ›´ã®å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_changes(\n",
    "    graph_client: GraphAPIClient,\n",
    "    drive_id: str,\n",
    "    delta_link: Optional[str] = None\n",
    ") -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Delta API ã§å¤‰æ›´ã‚’å–å¾—\n",
    "    \n",
    "    Returns:\n",
    "        (changes, new_delta_link)\n",
    "    \"\"\"\n",
    "    all_changes = []\n",
    "    \n",
    "    # åˆå›ã¾ãŸã¯deltaLinkãŒã‚ã‚‹å ´åˆ\n",
    "    if delta_link:\n",
    "        # æ—¢å­˜ã® deltaLink ã‚’ä½¿ç”¨\n",
    "        url = delta_link\n",
    "    else:\n",
    "        # åˆå›: delta ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨\n",
    "        url = f\"{graph_client.base_url}/drives/{drive_id}/root/delta\"\n",
    "    \n",
    "    while url:\n",
    "        if url.startswith(\"http\"):\n",
    "            response = graph_client.get_with_url(url)\n",
    "        else:\n",
    "            response = graph_client.get(url)\n",
    "        \n",
    "        # å¤‰æ›´ã‚’è¿½åŠ \n",
    "        all_changes.extend(response.get(\"value\", []))\n",
    "        \n",
    "        # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¾ãŸã¯ deltaLink\n",
    "        url = response.get(\"@odata.nextLink\")  # ã¾ã ç¶šããŒã‚ã‚‹å ´åˆ\n",
    "        if not url:\n",
    "            # ã™ã¹ã¦å–å¾—å®Œäº†: deltaLink ã‚’ä¿å­˜\n",
    "            new_delta_link = response.get(\"@odata.deltaLink\")\n",
    "            break\n",
    "    \n",
    "    return all_changes, new_delta_link\n",
    "\n",
    "def classify_changes(\n",
    "    changes: List[Dict],\n",
    "    supported_extensions: List[str]\n",
    ") -> Tuple[List[Dict], List[Dict], List[str]]:\n",
    "    \"\"\"\n",
    "    å¤‰æ›´ã‚’åˆ†é¡\n",
    "    \n",
    "    Returns:\n",
    "        (added_files, updated_files, deleted_ids)\n",
    "    \"\"\"\n",
    "    added = []\n",
    "    updated = []\n",
    "    deleted = []\n",
    "    \n",
    "    for item in changes:\n",
    "        # å‰Šé™¤ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ \n",
    "        if \"deleted\" in item or item.get(\"@removed\"):\n",
    "            deleted.append(item[\"id\"])\n",
    "            continue\n",
    "        \n",
    "        # ãƒ•ã‚©ãƒ«ãƒ€ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "        if \"folder\" in item:\n",
    "            continue\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "        if \"file\" not in item:\n",
    "            continue\n",
    "        \n",
    "        # ã‚µãƒãƒ¼ãƒˆå¯¾è±¡ã®æ‹¡å¼µå­ã‹ãƒã‚§ãƒƒã‚¯\n",
    "        name = item.get(\"name\", \"\")\n",
    "        ext = Path(name).suffix.lower()\n",
    "        \n",
    "        if ext not in supported_extensions:\n",
    "            continue\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’æ§‹é€ åŒ–\n",
    "        file_info = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"name\": name,\n",
    "            \"path\": item.get(\"parentReference\", {}).get(\"path\", \"\") + \"/\" + name,\n",
    "            \"extension\": ext,\n",
    "            \"size\": item.get(\"size\", 0),\n",
    "            \"webUrl\": item.get(\"webUrl\", \"\"),\n",
    "            \"lastModified\": item.get(\"lastModifiedDateTime\", \"\"),\n",
    "            \"createdBy\": item.get(\"createdBy\", {}).get(\"user\", {}).get(\"displayName\", \"\"),\n",
    "            \"modifiedBy\": item.get(\"lastModifiedBy\", {}).get(\"user\", {}).get(\"displayName\", \"\"),\n",
    "        }\n",
    "        \n",
    "        # æ–°è¦ã‹æ›´æ–°ã‹ã‚’åˆ¤å®š (createdDateTime ã¨ lastModifiedDateTime ã§åˆ¤å®š)\n",
    "        created = item.get(\"createdDateTime\", \"\")\n",
    "        modified = item.get(\"lastModifiedDateTime\", \"\")\n",
    "        \n",
    "        # ç°¡æ˜“åˆ¤å®š: ä½œæˆæ—¥æ™‚ã¨æ›´æ–°æ—¥æ™‚ãŒè¿‘ã‘ã‚Œã°æ–°è¦\n",
    "        if created and modified:\n",
    "            created_dt = datetime.fromisoformat(created.replace(\"Z\", \"+00:00\"))\n",
    "            modified_dt = datetime.fromisoformat(modified.replace(\"Z\", \"+00:00\"))\n",
    "            diff_seconds = (modified_dt - created_dt).total_seconds()\n",
    "            \n",
    "            if diff_seconds < 60:  # 1åˆ†ä»¥å†…ãªã‚‰æ–°è¦\n",
    "                added.append(file_info)\n",
    "            else:\n",
    "                updated.append(file_info)\n",
    "        else:\n",
    "            # åˆ¤å®šã§ããªã„å ´åˆã¯æ›´æ–°æ‰±ã„\n",
    "            updated.append(file_info)\n",
    "    \n",
    "    return added, updated, deleted\n",
    "\n",
    "print(\"âœ… Delta API é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222193c",
   "metadata": {},
   "source": [
    "## 5. å¤‰æ›´ã®å–å¾—ã¨åˆ†é¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c815256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…¨ãƒ‰ãƒ©ã‚¤ãƒ–ã®å¤‰æ›´ã‚’å–å¾—\n",
    "all_added = []\n",
    "all_updated = []\n",
    "all_deleted = []\n",
    "new_delta_tokens = {}\n",
    "\n",
    "print(\"\\nå¤‰æ›´ã‚’å–å¾—ã—ã¦ã„ã¾ã™...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for drive in config[\"target_drives\"]:\n",
    "    drive_id = drive[\"id\"]\n",
    "    drive_name = drive[\"name\"]\n",
    "    \n",
    "    print(f\"\\nğŸ“ {drive_name}\")\n",
    "    \n",
    "    # æ—¢å­˜ã® Delta Token ã‚’å–å¾—\n",
    "    delta_link = delta_tokens.get(drive_id)\n",
    "    \n",
    "    if delta_link:\n",
    "        print(\"   æ—¢å­˜ã® Delta Token ã‚’ä½¿ç”¨ã—ã¦å·®åˆ†ã‚’å–å¾—...\")\n",
    "    else:\n",
    "        print(\"   åˆå›å®Ÿè¡Œ: ã™ã¹ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—...\")\n",
    "    \n",
    "    try:\n",
    "        # Delta API å‘¼ã³å‡ºã—\n",
    "        changes, new_delta_link = get_delta_changes(\n",
    "            graph_client,\n",
    "            drive_id,\n",
    "            delta_link\n",
    "        )\n",
    "        \n",
    "        print(f\"   å–å¾—ã—ãŸå¤‰æ›´: {len(changes)} ä»¶\")\n",
    "        \n",
    "        # å¤‰æ›´ã‚’åˆ†é¡\n",
    "        added, updated, deleted = classify_changes(\n",
    "            changes,\n",
    "            config[\"supported_extensions\"]\n",
    "        )\n",
    "        \n",
    "        # ãƒ‰ãƒ©ã‚¤ãƒ–æƒ…å ±ã‚’è¿½åŠ \n",
    "        for f in added:\n",
    "            f[\"drive_id\"] = drive_id\n",
    "            f[\"drive_name\"] = drive_name\n",
    "        for f in updated:\n",
    "            f[\"drive_id\"] = drive_id\n",
    "            f[\"drive_name\"] = drive_name\n",
    "        \n",
    "        all_added.extend(added)\n",
    "        all_updated.extend(updated)\n",
    "        all_deleted.extend(deleted)\n",
    "        \n",
    "        # æ–°ã—ã„ Delta Token ã‚’ä¿å­˜\n",
    "        new_delta_tokens[drive_id] = new_delta_link\n",
    "        \n",
    "        print(f\"   âœ… è¿½åŠ : {len(added)} ä»¶\")\n",
    "        print(f\"   âœ… æ›´æ–°: {len(updated)} ä»¶\")\n",
    "        print(f\"   âœ… å‰Šé™¤: {len(deleted)} ä»¶\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š å¤‰æ›´ã‚µãƒãƒªãƒ¼\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«: {len(all_added)} ä»¶\")\n",
    "print(f\"æ›´æ–°ãƒ•ã‚¡ã‚¤ãƒ«: {len(all_updated)} ä»¶\")\n",
    "print(f\"å‰Šé™¤ãƒ•ã‚¡ã‚¤ãƒ«: {len(all_deleted)} ä»¶\")\n",
    "print(f\"åˆè¨ˆ: {len(all_added) + len(all_updated) + len(all_deleted)} ä»¶\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a21046f",
   "metadata": {},
   "source": [
    "## 6. Azure AI Search ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50466dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(search_key)\n",
    ")\n",
    "\n",
    "# Azure OpenAI (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)\n",
    "use_vector_search = bool(os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "openai_client = None\n",
    "embedding_deployment = None\n",
    "\n",
    "if use_vector_search:\n",
    "    try:\n",
    "        from openai import AzureOpenAI\n",
    "        \n",
    "        openai_client = AzureOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\")\n",
    "        )\n",
    "        embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "        print(f\"âœ… ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢: æœ‰åŠ¹ ({embedding_deployment})\")\n",
    "    except:\n",
    "        use_vector_search = False\n",
    "        print(\"âš ï¸  ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢: ç„¡åŠ¹\")\n",
    "else:\n",
    "    print(\"âš ï¸  ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢: ç„¡åŠ¹\")\n",
    "\n",
    "print(f\"\\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8727bf",
   "metadata": {},
   "source": [
    "## 7. ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–¢æ•° (A2 ã‹ã‚‰å†åˆ©ç”¨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4efee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(\n",
    "    graph_client: GraphAPIClient,\n",
    "    file_info: Dict,\n",
    "    site_info: Dict,\n",
    "    chunker: TextChunker,\n",
    "    openai_client = None,\n",
    "    embedding_deployment: str = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’è¿”ã™\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "        download_url = f\"/drives/{file_info['drive_id']}/items/{file_info['id']}/content\"\n",
    "        content_response = requests.get(\n",
    "            f\"{graph_client.base_url}{download_url}\",\n",
    "            headers=graph_client.get_headers(),\n",
    "            allow_redirects=True\n",
    "        )\n",
    "        content_response.raise_for_status()\n",
    "        content_bytes = content_response.content\n",
    "        \n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º\n",
    "        text = TextExtractor.extract(content_bytes, file_info[\"extension\"])\n",
    "        \n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return []  # ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "        \n",
    "        # ACLå–å¾— (æœ¬ç•ªç’°å¢ƒå®Ÿè£…)\n",
    "        acl_users = []\n",
    "        acl_groups = []\n",
    "        \n",
    "        try:\n",
    "            # Permissions APIã‚’å‘¼ã³å‡ºã—ã¦ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’å–å¾—\n",
    "            permissions_endpoint = f\"/drives/{file_info['drive_id']}/items/{file_info['id']}/permissions\"\n",
    "            permissions_response = graph_client.get(permissions_endpoint)\n",
    "            permissions = permissions_response.get(\"value\", [])\n",
    "            \n",
    "            # ACLã‚’æŠ½å‡º\n",
    "            acl_users, acl_groups = extract_acl_from_permissions(permissions)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Permissionå–å¾—ã«å¤±æ•—ã—ã¦ã‚‚å‡¦ç†ã‚’ç¶šè¡Œ\n",
    "            print(f\"âš ï¸  Permissionå–å¾—ã‚¨ãƒ©ãƒ¼ ({file_info['name']}): {str(e)[:80]}\")\n",
    "        \n",
    "        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²\n",
    "        chunks = chunker.split_text(text, metadata=file_info)\n",
    "        \n",
    "        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ\n",
    "        documents = []\n",
    "        for chunk in chunks:\n",
    "            doc_id = create_document_id(\n",
    "                site_info[\"id\"],\n",
    "                file_info[\"drive_id\"],\n",
    "                file_info[\"id\"],\n",
    "                chunk.chunk_index\n",
    "            )\n",
    "            \n",
    "            doc = {\n",
    "                \"id\": doc_id,\n",
    "                \"title\": file_info[\"name\"],\n",
    "                \"content\": chunk.text,\n",
    "                \"url\": file_info[\"webUrl\"],\n",
    "                \"path\": file_info[\"path\"],\n",
    "                \"site\": site_info[\"name\"],\n",
    "                \"library\": file_info[\"drive_name\"],\n",
    "                \"contentType\": file_info[\"extension\"][1:] if file_info[\"extension\"] else \"unknown\",\n",
    "                \"fileExtension\": file_info[\"extension\"],\n",
    "                \"lastModified\": file_info[\"lastModified\"],\n",
    "                \"createdBy\": file_info[\"createdBy\"],\n",
    "                \"modifiedBy\": file_info[\"modifiedBy\"],\n",
    "                \"size\": file_info[\"size\"],\n",
    "                \"chunkIndex\": chunk.chunk_index,\n",
    "                \"aclUsers\": acl_users,\n",
    "                \"aclGroups\": acl_groups,\n",
    "            }\n",
    "            \n",
    "            # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)\n",
    "            if openai_client and embedding_deployment:\n",
    "                try:\n",
    "                    response = openai_client.embeddings.create(\n",
    "                        input=chunk.text[:8000],  # ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–\n",
    "                        model=embedding_deployment\n",
    "                    )\n",
    "                    doc[\"contentVector\"] = response.data[0].embedding\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼ ({file_info['name']}): {str(e)[:50]}\")\n",
    "            \n",
    "            documents.append(doc)\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({file_info['name']}): {str(e)[:100]}\")\n",
    "        return []\n",
    "\n",
    "# ãƒãƒ£ãƒ³ã‚«ãƒ¼ã®æº–å‚™\n",
    "chunker = TextChunker(\n",
    "    chunk_size=config[\"chunk_size\"],\n",
    "    chunk_overlap=config[\"chunk_overlap\"]\n",
    ")\n",
    "\n",
    "print(\"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "print(f\"   ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {config['chunk_size']} æ–‡å­—\")\n",
    "print(f\"   ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: {config['chunk_overlap']} æ–‡å­—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e12b9",
   "metadata": {},
   "source": [
    "## 8. è¿½åŠ ãƒ»æ›´æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ddf8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿½åŠ ãƒ»æ›´æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã¾ã¨ã‚ã¦å‡¦ç†\n",
    "files_to_process = all_added + all_updated\n",
    "\n",
    "if files_to_process:\n",
    "    print(f\"\\n{len(files_to_process)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™...\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_documents = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_info in tqdm(files_to_process, desc=\"å‡¦ç†ä¸­\"):\n",
    "        docs = process_file(\n",
    "            graph_client,\n",
    "            file_info,\n",
    "            config[\"site_info\"],\n",
    "            chunker,\n",
    "            openai_client,\n",
    "            embedding_deployment\n",
    "        )\n",
    "        \n",
    "        if docs:\n",
    "            all_documents.extend(docs)\n",
    "        else:\n",
    "            failed_files.append(file_info[\"name\"])\n",
    "        \n",
    "        # APIåˆ¶é™å¯¾ç­–\n",
    "        if use_vector_search:\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"âœ… å‡¦ç†å®Œäº†: {len(all_documents)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ\")\n",
    "    if failed_files:\n",
    "        print(f\"âš ï¸  å¤±æ•—: {len(failed_files)} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "    if all_documents:\n",
    "        print(\"\\nAzure AI Search ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™...\")\n",
    "        \n",
    "        batch_size = 100\n",
    "        total_uploaded = 0\n",
    "        total_failed = 0\n",
    "        \n",
    "        for i in tqdm(range(0, len(all_documents), batch_size), desc=\"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­\"):\n",
    "            batch = all_documents[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                result = search_client.upload_documents(documents=batch)\n",
    "                \n",
    "                for res in result:\n",
    "                    if res.succeeded:\n",
    "                        total_uploaded += 1\n",
    "                    else:\n",
    "                        total_failed += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                total_failed += len(batch)\n",
    "                print(f\"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}\")\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        print(f\"\\nâœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {total_uploaded} / {len(all_documents)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\")\n",
    "        if total_failed > 0:\n",
    "            print(f\"âš ï¸  å¤±æ•—: {total_failed} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\")\n",
    "else:\n",
    "    print(\"\\nâ„¹ï¸  è¿½åŠ ãƒ»æ›´æ–°ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d30c36",
   "metadata": {},
   "source": [
    "## 9. å‰Šé™¤ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_deleted:\n",
    "    print(f\"\\n{len(all_deleted)} ä»¶ã®å‰Šé™¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™...\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # å‰Šé™¤å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ID ã‚’ç”Ÿæˆ\n",
    "    # SharePoint ã®ãƒ•ã‚¡ã‚¤ãƒ« ID ã‚’ã‚‚ã¨ã«ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†…ã®é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢ã—ã¦å‰Šé™¤\n",
    "    deleted_count = 0\n",
    "    \n",
    "    for file_id in tqdm(all_deleted, desc=\"å‰Šé™¤ä¸­\"):\n",
    "        try:\n",
    "            # ãƒ•ã‚¡ã‚¤ãƒ« ID ã‚’å«ã‚€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢\n",
    "            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ID ã®ãƒ‘ã‚¿ãƒ¼ãƒ³: {site_id}_{drive_id}_{file_id}_{chunk_index}\n",
    "            # ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã¯ä½¿ãˆãªã„ã®ã§ã€filter ã§æ¤œç´¢\n",
    "            \n",
    "            # ç°¡æ˜“çš„ãªæ–¹æ³•: id ã«ãƒ•ã‚¡ã‚¤ãƒ« ID ãŒå«ã¾ã‚Œã‚‹ã‚‚ã®ã‚’æ¤œç´¢\n",
    "            # (æœ¬ç•ªç’°å¢ƒã§ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ« ID ã‚’åˆ¥ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ç®¡ç†ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨)\n",
    "            search_results = search_client.search(\n",
    "                search_text=\"*\",\n",
    "                filter=f\"search.in(id, '{file_id}', '_')\",  # éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ã®ä»£æ›¿\n",
    "                select=[\"id\"],\n",
    "                top=1000\n",
    "            )\n",
    "            \n",
    "            docs_to_delete = []\n",
    "            for result in search_results:\n",
    "                # ID ã«ãƒ•ã‚¡ã‚¤ãƒ« ID ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "                if file_id in result[\"id\"]:\n",
    "                    docs_to_delete.append({\"id\": result[\"id\"]})\n",
    "            \n",
    "            if docs_to_delete:\n",
    "                # å‰Šé™¤å®Ÿè¡Œ\n",
    "                delete_result = search_client.delete_documents(documents=docs_to_delete)\n",
    "                deleted_count += len([r for r in delete_result if r.succeeded])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å‰Šé™¤ã‚¨ãƒ©ãƒ¼ ({file_id}): {str(e)[:100]}\")\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"âœ… å‰Šé™¤å®Œäº†: {deleted_count} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"\\nâ„¹ï¸  å‰Šé™¤ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c0d47",
   "metadata": {},
   "source": [
    "## 10. Delta Token ã®ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad975866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–°ã—ã„ Delta Token ã‚’ä¿å­˜\n",
    "save_delta_tokens(new_delta_tokens)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… å·®åˆ†åŒæœŸãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nå‡¦ç†ã‚µãƒãƒªãƒ¼:\")\n",
    "print(f\"  è¿½åŠ : {len(all_added)} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "print(f\"  æ›´æ–°: {len(all_updated)} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "print(f\"  å‰Šé™¤: {len(all_deleted)} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "print(f\"\\næ¬¡å›å®Ÿè¡Œæ™‚ã¯ã€ã“ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ä»¥é™ã®å¤‰æ›´ã®ã¿ãŒå‡¦ç†ã•ã‚Œã¾ã™\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4ac6a",
   "metadata": {},
   "source": [
    "## 11. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆã®ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc989cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆ\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    results = search_client.search(\n",
    "        search_text=\"*\",\n",
    "        top=1,\n",
    "        include_total_count=True\n",
    "    )\n",
    "    \n",
    "    total_count = results.get_count()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“Š ç¾åœ¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆ\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å: {index_name}\")\n",
    "    print(f\"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {total_count:,} ä»¶\")\n",
    "    print(f\"æœ€çµ‚æ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a202a",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ä»¥ä¸‹ã‚’å®Œäº†ã—ã¾ã—ãŸ:\n",
    "\n",
    "âœ… Delta API ã«ã‚ˆã‚‹å¤‰æ›´ã®æ¤œå‡º  \n",
    "âœ… è¿½åŠ ãƒ»æ›´æ–°ãƒ»å‰Šé™¤ã®åˆ†é¡  \n",
    "âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å¢—åˆ†æ›´æ–°  \n",
    "âœ… Delta Token ã®ä¿å­˜  \n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "**A4_query_and_acl_demo.ipynb** ã«é€²ã‚“ã§ã€æ¤œç´¢æ©Ÿèƒ½ã¨ ACL ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è©¦ã—ã¾ã™ã€‚\n",
    "\n",
    "### å®šæœŸå®Ÿè¡Œã®è¨­å®š\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®šæœŸå®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€å¸¸ã«æœ€æ–°ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç¶­æŒã§ãã¾ã™ã€‚\n",
    "\n",
    "**å®Ÿè¡Œæ–¹æ³•ã®ä¾‹**:\n",
    "\n",
    "1. **Azure Functions + Timer Trigger**: \n",
    "   - ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«å¤‰æ›\n",
    "   - 5åˆ†ã€œ1æ™‚é–“ã”ã¨ã«å®Ÿè¡Œ\n",
    "\n",
    "2. **GitHub Actions**:\n",
    "   - Papermill ã§ Notebook ã‚’å®Ÿè¡Œ\n",
    "   - cron ã§å®šæœŸå®Ÿè¡Œ\n",
    "\n",
    "3. **Azure Data Factory**:\n",
    "   - Notebook Activity ã§å®Ÿè¡Œ\n",
    "   - ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« ãƒˆãƒªã‚¬ãƒ¼ã§å®šæœŸå®Ÿè¡Œ\n",
    "\n",
    "### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ’ãƒ³ãƒˆ\n",
    "\n",
    "**åˆå›å®Ÿè¡Œ**:\n",
    "- ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã™ã‚‹ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™\n",
    "- ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå¤šã„å ´åˆã¯ã€ãƒ‰ãƒ©ã‚¤ãƒ–ã”ã¨ã«åˆ†å‰²å®Ÿè¡Œã‚’æ¨å¥¨\n",
    "\n",
    "**2å›ç›®ä»¥é™**:\n",
    "- Delta Token ã«ã‚ˆã‚Šå¤‰æ›´ã®ã¿ã‚’å‡¦ç†\n",
    "- é€šå¸¸ã¯æ•°ç§’ã€œæ•°åˆ†ã§å®Œäº†\n",
    "\n",
    "**æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ**:\n",
    "- ä¸¦åˆ—å‡¦ç†: `concurrent.futures.ThreadPoolExecutor`\n",
    "- ãƒãƒƒãƒã‚µã‚¤ã‚º: API ã®åˆ¶é™ã«åˆã‚ã›ã¦èª¿æ•´\n",
    "- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã®å¼·åŒ–\n",
    "\n",
    "### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "\n",
    "**Delta Token ã‚¨ãƒ©ãƒ¼**:\n",
    "- Token ãŒå¤ã™ãã‚‹å ´åˆã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™\n",
    "- `.delta_token.json` ã‚’å‰Šé™¤ã—ã¦å†å®Ÿè¡Œ\n",
    "\n",
    "**å‰Šé™¤å‡¦ç†ã®æ”¹å–„**:\n",
    "- ç¾åœ¨ã®å®Ÿè£…ã¯ç°¡æ˜“ç‰ˆã§ã™\n",
    "- æœ¬ç•ªç’°å¢ƒã§ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ« ID ã‚’å°‚ç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ç®¡ç†ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨\n",
    "- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¹ã‚­ãƒ¼ãƒã« `sourceFileId` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ \n",
    "\n",
    "**å¤‰æ›´ã®æ¤œå‡ºæ¼ã‚Œ**:\n",
    "- Delta API ã¯æ¨©é™å¤‰æ›´ã®ã¿ã§ã‚‚ã‚¤ãƒ™ãƒ³ãƒˆã‚’è¿”ã—ã¾ã™\n",
    "- å¿…è¦ã«å¿œã˜ã¦ã€æ¨©é™å¤‰æ›´æ™‚ã® ACL æ›´æ–°å‡¦ç†ã‚’è¿½åŠ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
